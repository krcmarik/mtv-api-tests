# MTV API Tests - Claude Instructions

## AI Workflow (MANDATORY)

**This is the required workflow for all code changes to this repository:**

1. **User Prompt** → User requests fix/new test/feature/enhancement
2. **Agent Selection** - AI analyzes request and triggers appropriate specialist agent:
   - Python code - `python-expert`
   - Git operations - `git-expert`
   - Documentation - `technical-documentation-writer`
   - Other specialists as needed
3. **Code Changes** - Specialist agent implements the changes
4. **Code Review (ALWAYS)** - After ANY code change, AUTOMATICALLY trigger `code-reviewer`
5. **Review Cycle** - If code-reviewer makes additional changes:
   - Return to step 4 (trigger code-reviewer again)
   - Repeat until no more changes are needed
6. **Completion** - All changes reviewed and approved

**CRITICAL RULES:**

- **MANDATORY:** ALL code changes MUST use specialist agents - NEVER modify code directly
- **MANDATORY:** Python code - `python-expert` agent
- **MANDATORY:** Git operations - `git-expert` agent
- **MANDATORY:** Documentation - `technical-documentation-writer` agent
- **MANDATORY:** NEVER work on main branch - ALWAYS create a feature branch first
- **MANDATORY:** Update README.md when code changes affect usage, requirements, installation, or configuration
- **MANDATORY:** Run agents in PARALLEL when possible - launch all independent agents in a SINGLE message
- **MANDATORY:** After ANY code change - `code-reviewer` agent (AUTOMATIC)
- **NEVER** skip code-reviewer after code changes
- **ALWAYS** repeat the review cycle if code-reviewer makes changes
- **PROHIBITED:** Direct use of Edit/Write tools for code files - agents ONLY

## CRITICAL: Agent Usage is MANDATORY

**ALL code operations MUST be performed through specialist agents. Direct code modification is STRICTLY
PROHIBITED.**

### When MUST You Use Agents

| Operation                    | Agent Required                   | Example                                   |
| ---------------------------- | -------------------------------- | ----------------------------------------- |
| Python code (any .py file)   | `python-expert`                  | Write test, fix bug, add utility function |
| Git operations               | `git-expert`                     | Commit, branch, push, create PR           |
| Documentation (README, docs) | `technical-documentation-writer` | Update docs, write guides                 |
| Code review                  | `code-reviewer`                  | Review after ANY code change              |

### What is FORBIDDEN

- ❌ **NEVER** use `Edit` tool on `.py` files directly
- ❌ **NEVER** use `Write` tool to create `.py` files directly
- ❌ **NEVER** modify code without routing to appropriate agent
- ❌ **NEVER** skip the agent → code-reviewer workflow

### What is ALLOWED

- ✅ Use `Read` tool to read any file
- ✅ Use `Grep` to search code
- ✅ Use `Glob` to find files
- ✅ Use agents for ALL code modifications

### Enforcement

If you modify code directly instead of using an agent:

1. **VIOLATION** - Stop immediately
2. Undo the changes
3. Route to the correct agent
4. Follow the proper workflow

**Remember:** Analysis and reading = OK. Code changes = AGENT REQUIRED.

## MTV API Tests Specific Guidelines

### Code Quality Requirements

- **Type Annotations:** Always use built-in Python typing (dict, list, tuple, etc.)
- **Package Management:** Use `uv` for all dependency and project management
- **OpenShift Integration:** Use `openshift-python-wrapper` for all cluster interactions (see detailed section below)
- **Pre-commit:** Must pass before any commit - never use `git commit --no-verify`
- **Code Simplicity:** Keep code simple and readable, avoid over-engineering
- Every openshift resource must be created using `create_and_store_resource` function only.

#### CRITICAL: OpenShift/Kubernetes Resource Interactions

**ALL OpenShift and Kubernetes resource interactions MUST use `openshift-python-wrapper` package.**

This is a **MANDATORY** requirement with no exceptions for direct kubernetes package usage at runtime.

**Correct imports:**

```python
# ✅ CORRECT - Use openshift-python-wrapper classes
from ocp_resources.namespace import Namespace
from ocp_resources.secret import Secret
from ocp_resources.virtual_machine import VirtualMachine
from ocp_resources.resource import Resource

# ✅ CORRECT - Get DynamicClient instance via ocp_utilities
from ocp_utilities.infra import get_client  # Returns a DynamicClient instance
```

**Forbidden imports (runtime usage):**

```python
# ❌ FORBIDDEN - Direct kubernetes package usage at runtime
from kubernetes import client
from kubernetes.client import CoreV1Api, CustomObjectsApi
from kubernetes.config import load_kube_config
import kubernetes

# ❌ FORBIDDEN - Instantiating DynamicClient directly
from kubernetes.dynamic import DynamicClient
client = DynamicClient(...)  # Never do this - use get_client() instead
```

**Why this rule exists:**

- `openshift-python-wrapper` provides consistent, tested abstractions for OpenShift resources
- It handles OpenShift-specific resources (Routes, DeploymentConfigs, etc.) that kubernetes package does not
- Resource lifecycle management (deploy, wait, delete) is standardized
- Integration with `create_and_store_resource()` for automatic cleanup tracking

**What to do:**

- ✅ **ALWAYS** use `ocp_resources.*` for resource classes
- ✅ **ALWAYS** use `ocp_utilities.*` for cluster utilities (including `get_client()`)
- ✅ **ALWAYS** use `create_and_store_resource()` for resource creation
- ❌ **NEVER** import directly from `kubernetes` package for runtime usage
- ❌ **NEVER** use `kubernetes.client.*` APIs
- ❌ **NEVER** instantiate `kubernetes.dynamic.DynamicClient` directly

**Commonly re-exported symbols from openshift-python-wrapper:**

The following resource classes are available from `ocp_resources` and should be used directly:

| Symbol           | Import Path                                                |
| ---------------- | ---------------------------------------------------------- |
| `Resource`       | `from ocp_resources.resource import Resource`              |
| `Namespace`      | `from ocp_resources.namespace import Namespace`            |
| `Secret`         | `from ocp_resources.secret import Secret`                  |
| `Pod`            | `from ocp_resources.pod import Pod`                        |
| `VirtualMachine` | `from ocp_resources.virtual_machine import VirtualMachine` |
| `StorageClass`   | `from ocp_resources.storage_class import StorageClass`     |
| `ConfigMap`      | `from ocp_resources.configmap import ConfigMap`            |
| `Service`        | `from ocp_resources.service import Service`                |

**Verifying re-exports:**

To check if a symbol is publicly available from `ocp_resources`, inspect the package structure:

```python
# Check available modules in ocp_resources
import ocp_resources
print(dir(ocp_resources))  # Lists available submodules

# Check a specific module's exports
from ocp_resources import namespace
print(dir(namespace))  # Should show 'Namespace' class
```

Look for class names that match Kubernetes/OpenShift resource kinds (e.g., `Namespace`, `Pod`, `Secret`).
Each resource module typically exports a single class with the resource name.

**Important clarification about get_client() and DynamicClient:**

- `ocp_utilities.infra.get_client()` returns a `kubernetes.dynamic.DynamicClient` instance
- This does NOT mean `DynamicClient` is re-exported by `openshift-python-wrapper`
- The fact that `get_client()` returns `DynamicClient` only permits importing `DynamicClient`
  for type annotations (see below)
- You must NEVER instantiate `DynamicClient` directly - always use `get_client()`

**Type annotations exception for DynamicClient:**

Importing `kubernetes.dynamic.DynamicClient` is permitted ONLY for type annotations because:

1. `ocp_utilities.infra.get_client()` returns a `DynamicClient` instance
2. `Resource` classes from `ocp_resources` accept `DynamicClient` as their `client` parameter
3. Type annotations require referencing the actual type

**Preferred pattern - Use TYPE_CHECKING to avoid runtime import:**

```python
from __future__ import annotations
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from kubernetes.dynamic import DynamicClient

from ocp_utilities.infra import get_client
from ocp_resources.namespace import Namespace

def create_namespace(client: DynamicClient, name: str) -> Namespace:
    """Type annotation using DynamicClient is allowed via TYPE_CHECKING."""
    return Namespace(client=client, name=name)

# Get the client instance via the approved method
admin_client = get_client()  # Returns DynamicClient instance
```

**Alternative - String annotations (also acceptable):**

Note: When using explicit string annotations like `"DynamicClient"`, the
`from __future__ import annotations` import is optional since the type is already a string literal.

```python
from ocp_utilities.infra import get_client
from ocp_resources.namespace import Namespace

def create_namespace(client: "DynamicClient", name: str) -> Namespace:
    """String annotation avoids runtime import."""
    return Namespace(client=client, name=name)
```

**What is FORBIDDEN even with the type annotation exception:**

```python
# ❌ FORBIDDEN - Runtime import and instantiation
from kubernetes.dynamic import DynamicClient
from kubernetes import config

config.load_kube_config()
client = DynamicClient(...)  # Never instantiate directly

# ❌ FORBIDDEN - Using DynamicClient for anything other than type hints
from kubernetes.dynamic import DynamicClient
isinstance(obj, DynamicClient)  # Runtime usage is forbidden
```

**Summary of DynamicClient rules:**

| Usage                                        | Allowed?                         |
| -------------------------------------------- | -------------------------------- |
| Import inside `TYPE_CHECKING` block          | Yes                              |
| String annotation `"DynamicClient"`          | Yes                              |
| Top-level import used only in annotations    | Yes (but prefer TYPE_CHECKING)   |
| Instantiate `DynamicClient(...)` directly    | No - use `get_client()`          |
| Use in `isinstance()` or runtime checks      | No                               |
| Import other `kubernetes.*` modules          | No                               |

#### Function Size - Keep Functions Small

**Large functions MUST be refactored into smaller, focused functions.**

- **Maximum recommended size:** ~50 lines per function
- **Single responsibility:** Each function should do ONE thing well
- **Extract helpers:** Use private helper functions (prefixed with `_`) for sub-tasks
- **Clear names:** Helper function names should describe what they do

**Example refactoring pattern:**

```python
# ❌ WRONG - One large function doing everything
def do_complex_task():
    # 200 lines of code doing multiple things
    pass

# ✅ CORRECT - Small functions with clear responsibilities
def _step_one() -> Result:
    """Handle first step."""
    ...

def _step_two(input: Result) -> Output:
    """Handle second step."""
    ...

def do_complex_task() -> Output:
    """Orchestrate the complex task."""
    result = _step_one()
    return _step_two(result)
```

### Testing Standards

- **Real Environments:** Tests run against real clusters and real providers
- **Provider Support:** VMware vSphere, RHV, OpenStack, OVA, OpenShift
- **Migration Types:** Both cold and warm migration testing
- **Resource Management:** Comprehensive fixture-based cleanup and teardown
- **Parallel Execution:** Support for pytest-xdist concurrent testing

### Development Workflow

- **Package Installation:** `uv sync`
- **Test Execution:** `uv run pytest <options>` (on live clusters only - AI cannot run tests)
- **Linting:** `ruff check` and `ruff format`
- **Type Checking:** `mypy` with strict configuration
- **Container Build:** `podman build -f Dockerfile -t mtv-api-tests`

### Configuration Management

- **Provider Configuration:** `.providers.json` file in project root
- **Test Configuration:** pytest-testconfig for test parameters
- **Environment Variables:** OpenShift cluster and provider authentication
- **Storage Classes:** Configurable via test configuration (nfs, csi, etc.)

## Architecture Patterns

### Provider Abstraction

- **Base Class:** `BaseProvider` in `libs/base_provider.py`
- **Provider Implementations:** VMware, RHV, OpenStack, OVA, OpenShift providers
- **Unified VM Representation:** Consistent VM data structure across providers
- **Connection Management:** Context manager support for provider connections

### Test Structure

- **Fixtures:** Session-scoped fixtures for resource management in `conftest.py`
- **Parametrization:** Provider-specific test parametrization
- **Markers:** tier0, warm, remote, copyoffload markers for test categorization
- **Data Collection:** Automatic log and must-gather collection on failures

### Resource Management

- **Cleanup Strategy:** Comprehensive teardown with fixture tracking
- **Namespace Management:** Unique namespace generation per test session
- **Resource Tracking:** JSON-based resource tracking for post-test cleanup
- **Error Recovery:** Must-gather data collection on test failures

## Critical Constraints and Patterns

### CRITICAL: Test Execution Prohibition

**AI must NEVER run tests directly.**

- ❌ **FORBIDDEN:** `pytest`, `uv run pytest`, any test execution commands
- ✅ **ALLOWED:** Analyze tests, write tests, fix tests, read test output

**Why:** Tests require:

- Live OpenShift cluster with MTV operator installed
- Real provider connections (VMware vSphere, RHV, OpenStack, etc.)
- Network access to source infrastructure
- Configured credentials and authentication

**AI can:**

- Read and analyze test code
- Write new tests following patterns (via `python-expert` agent)
- Fix failing tests based on logs (via `python-expert` agent)
- Suggest test improvements
- Review test structure (via `code-reviewer` agent)

**AI cannot:**

- Execute pytest commands
- Run individual tests
- Validate tests by running them
- Modify code directly (must use agents)

### CRITICAL: Deterministic Tests - No Defaults/Fallbacks

**All code must be deterministic with NO assumptions or default values for configurations WE CONTROL.**

**This rule applies to configurations defined in our codebase:**

- `py_config` - test configuration we define in `tests/tests_config/config.py`
- `plan` - test plan parameters we define in test parametrization
- `tests_params` - test parameters dictionary we define
- Any configuration dictionary or structure defined in our codebase

**For OUR configurations - NEVER use defaults:**

```python
# ❌ WRONG - Uses fallback/default for OUR configuration
storage_class = py_config.get("storage_class", "default-storage")
vm_name = plan.get("vm_name", "default-vm")
timeout = py_config.get("plan_wait_timeout", 3600)

# ✅ CORRECT - Deterministic, fails fast with KeyError
storage_class = py_config["storage_class"]
vm_name = plan["virtual_machines"][0]["name"]
timeout = py_config["plan_wait_timeout"]
```

**For external/provider configurations - `.get()` with validation is acceptable:**

```python
# ✅ ACCEPTABLE - External config with explicit validation and meaningful error
vm_id = provider_data.get("vm_id")
if not vm_id:
    raise ValueError(f"VM ID not found in provider data for VM '{vm_name}'.")

# ✅ ACCEPTABLE - Provider API response with validation
disk_type = vm_details.get("disk_type")
if disk_type not in ["thin", "thick-lazy", "thick-eager"]:
    raise ValueError(f"Unsupported disk type '{disk_type}' for VM '{vm_name}'")
```

**Why this distinction:**

- **Our config:** We control it, should be complete, fail fast if missing (KeyError)
- **External data:** We don't control it, explicit validation provides better error messages than raw KeyError
- Tests must fail immediately if OUR configuration is incomplete
- External data validation should provide context-rich error messages

**Exception - Optional feature flags in OUR config:**

```python
# ✅ OK - Optional boolean feature flag in our config
if plan.get("warm_migration", False):  # Optional feature, defaults to False
    setup_warm_migration()

# ✅ OK - Optional list in our config
hooks = plan.get("pre_migration_hooks", [])  # Optional hooks, defaults to empty
for hook in hooks:
    execute_hook(hook)
```

### Resource Creation Pattern - MANDATORY

**CRITICAL: Every OpenShift resource MUST be created using `create_and_store_resource()` function.**

**Location:** `utilities/resources.py:create_and_store_resource()`

**Why This Pattern:**

- Automatic resource tracking for cleanup
- Unique name generation with UUID (prevents conflicts in parallel execution)
- Handles resource conflicts gracefully (reuses existing)
- Ensures cleanup even when tests fail or crash
- Truncates names to Kubernetes 63-character limit
- Thread-safe for parallel test execution (pytest-xdist)

**Function Signature:**

```python
def create_and_store_resource(
    client: DynamicClient,
    fixture_store: dict[str, Any],
    resource: type[Resource],
    test_name: str | None = None,
    **kwargs: Any,
) -> Any:
```

**Correct Usage Examples:**

```python
# Namespace creation
namespace = create_and_store_resource(
    fixture_store=fixture_store,
    resource=Namespace,
    client=ocp_admin_client,
    name="my-namespace",
    label={"app": "mtv-tests"},
)

# Provider creation
provider = create_and_store_resource(
    fixture_store=fixture_store,
    resource=Provider,
    client=ocp_admin_client,
    namespace=target_namespace,
    secret_name=secret.name,
    url=provider_url,
    provider_type=Provider.ProviderType.VSPHERE,
)

# Secret creation
secret = create_and_store_resource(
    client=ocp_admin_client,
    fixture_store=fixture_store,
    resource=Secret,
    namespace=target_namespace,
    string_data={
        "user": username,
        "password": password,
        "insecureSkipVerify": "true",
    },
)

# NetworkAttachmentDefinition
nad = create_and_store_resource(
    fixture_store=fixture_store,
    resource=NetworkAttachmentDefinition,
    client=ocp_admin_client,
    namespace=target_namespace,
    name="bridge-network",
    cni_type="bridge",
    config=multus_config,
)
```

**What NEVER to do:**

```python
# ❌ WRONG - Direct resource instantiation bypasses tracking
namespace = Namespace(client=ocp_admin_client, name="my-namespace")
namespace.deploy()

# ❌ WRONG - No cleanup tracking
provider = Provider(
    client=ocp_admin_client,
    namespace=target_namespace,
    secret_name=secret.name,
)
provider.deploy(wait=True)

# ❌ WRONG - Manual resource creation
secret_dict = {...}
Secret(**secret_dict).deploy()
```

**Key Features:**

1. **Auto-generates unique names** using session UUID if name not provided
2. **Deploys resource** and waits for readiness by default
3. **Stores metadata** in `fixture_store["teardown"]` for cleanup
4. **Handles conflicts** - reuses existing resources if already created
5. **Name truncation** - ensures names don't exceed 63 chars (Kubernetes limit)
6. **Test tracking** - optional `test_name` for test-specific resources

### Test Structure Pattern - How to Add New Tests

**ALL tests in this repository follow the same structure.**

**Standard Test Pattern:**

```python
from pytest_testconfig import config as py_config
from utilities.mtv_migration import create_storagemap_and_networkmap, migrate_vms

@pytest.mark.parametrize(
    "plan",
    [pytest.param(py_config["tests_params"]["test_name_here"])],
    indirect=True,  # REQUIRED - passes to plan fixture first
    ids=["descriptive-test-id"],
)
@pytest.mark.tier0  # optional marker
@pytest.mark.warm   # optional: warm/remote/copyoffload
def test_name_here(
    request,                    # pytest request object
    fixture_store,              # resource tracking dictionary
    ocp_admin_client,           # OpenShift DynamicClient
    target_namespace,           # test namespace
    destination_provider,       # destination provider
    plan,                       # test plan from parametrize
    source_provider,            # source provider connection
    source_provider_data,       # source provider config
    multus_network_name,        # multus network name
    source_provider_inventory,  # provider inventory
    source_vms_namespace,       # source VMs namespace
):
    # 1. Create storage and network migration maps
    storage_migration_map, network_migration_map = create_storagemap_and_networkmap(
        fixture_store=fixture_store,
        source_provider=source_provider,
        destination_provider=destination_provider,
        source_provider_inventory=source_provider_inventory,
        ocp_admin_client=ocp_admin_client,
        multus_network_name=multus_network_name,
        target_namespace=target_namespace,
        plan=plan,
    )

    # 2. Execute migration
    migrate_vms(
        ocp_admin_client=ocp_admin_client,
        request=request,
        fixture_store=fixture_store,
        source_provider=source_provider,
        destination_provider=destination_provider,
        plan=plan,
        network_migration_map=network_migration_map,
        storage_migration_map=storage_migration_map,
        source_provider_data=source_provider_data,
        target_namespace=target_namespace,
        source_vms_namespace=source_vms_namespace,
        source_provider_inventory=source_provider_inventory,
    )
```

**Step-by-Step Guide to Add New Test:**

1. **Add test configuration** to `tests/tests_config/config.py`:

   ```python
   tests_params: dict = {
       "test_my_new_test": {
           "virtual_machines": [
               {
                   "name": "vm-name-in-source",
                   "source_vm_power": "on",  # or "off"
                   "guest_agent": True,      # if guest agent installed
               },
           ],
           "warm_migration": False,  # True for warm, False for cold
       },
   }
   ```

2. **Create test file** in `tests/` directory:
   - File naming: `test_<feature>_migration.py`
   - Example: `test_my_feature_migration.py`

3. **Import required modules** (see example above)

4. **Add parametrize decorator** with your test config:

   ```python
   @pytest.mark.parametrize(
       "plan",
       [pytest.param(py_config["tests_params"]["test_my_new_test"])],
       indirect=True,
       ids=["my-test-id"],
   )
   ```

5. **Add pytest markers**:
   - **Optional:** `@pytest.mark.tier0` (core functionality tests)
   - **Optional:** `@pytest.mark.warm`, `@pytest.mark.remote`, `@pytest.mark.copyoffload`

6. **Define test function** with standard fixtures (see example above)

7. **Follow the two-step pattern:**
   - Step 1: Create migration maps
   - Step 2: Execute migration

### Example: Adding a new warm migration test

File: `tests/test_my_warm_migration.py`

```python
import pytest
from pytest_testconfig import config as py_config

from utilities.mtv_migration import create_storagemap_and_networkmap, migrate_vms


@pytest.mark.parametrize(
    "plan",
    [pytest.param(py_config["tests_params"]["test_my_warm_migration"])],
    indirect=True,
    ids=["warm-rhel9"],
)
@pytest.mark.tier0
@pytest.mark.warm
def test_my_warm_migration(
    request,
    fixture_store,
    ocp_admin_client,
    target_namespace,
    destination_provider,
    plan,
    source_provider,
    source_provider_data,
    multus_network_name,
    source_provider_inventory,
    source_vms_namespace,
):
    storage_migration_map, network_migration_map = create_storagemap_and_networkmap(
        fixture_store=fixture_store,
        source_provider=source_provider,
        destination_provider=destination_provider,
        source_provider_inventory=source_provider_inventory,
        ocp_admin_client=ocp_admin_client,
        multus_network_name=multus_network_name,
        target_namespace=target_namespace,
        plan=plan,
    )

    migrate_vms(
        ocp_admin_client=ocp_admin_client,
        request=request,
        fixture_store=fixture_store,
        source_provider=source_provider,
        destination_provider=destination_provider,
        plan=plan,
        network_migration_map=network_migration_map,
        storage_migration_map=storage_migration_map,
        source_provider_data=source_provider_data,
        target_namespace=target_namespace,
        source_vms_namespace=source_vms_namespace,
        source_provider_inventory=source_provider_inventory,
    )
```

**DO NOT:**

- ❌ Create custom test structures
- ❌ Skip parametrization
- ❌ Hardcode VM names in test (use config)
- ❌ Create resources without `create_and_store_resource()`
- ❌ Skip feature markers when applicable (warm, remote, copyoffload)

### Test Configuration Pattern

**All test configurations are centralized in `tests/tests_config/config.py`.**

**Configuration Structure:**

```python
global config

# Global settings (module-level variables)
insecure_verify_skip: str = "true"
number_of_vms: int = 1
check_vms_signals: bool = True
target_namespace_prefix: str = "auto"
mtv_namespace: str = "openshift-mtv"
plan_wait_timeout: int = 3600
remote_ocp_cluster: str = ""

# Test-specific parameters
tests_params: dict = {
    "test_name": {
        "virtual_machines": [
            {
                "name": "vm-name",
                "source_vm_power": "on",  # "on" or "off"
                "guest_agent": True,      # True if GA installed
            },
        ],
        "warm_migration": False,  # True for warm, False for cold
    },
}

# Auto-export to pytest config (DO NOT MODIFY THIS SECTION)
for _dir in dir():
    val = locals()[_dir]
    if type(val) not in [bool, list, dict, str, int]:
        continue
    if _dir in ["encoding", "py_file", "__annotations__"]:
        continue
    config[_dir] = locals()[_dir]
```

**How to Add New Test Configuration:**

1. **Add entry to `tests_params` dictionary**:

   ```python
   tests_params: dict = {
       # Existing tests...

       # Your new test
       "test_my_new_feature": {
           "virtual_machines": [
               {
                   "name": "source-vm-name",
                   "source_vm_power": "on",
                   "guest_agent": True,
               },
           ],
           "warm_migration": False,
       },
   }
   ```

2. **VM configuration options:**

   ```python
   {
       "name": "vm-name",              # REQUIRED: VM name in source provider
       "source_vm_power": "on",         # Optional: "on" or "off"
       "guest_agent": True,             # Optional: True if guest agent installed
       "clone": True,                   # Optional: Clone VM before migration
       "disk_type": "thin",             # Optional: "thin", "thick-lazy", "thick-eager"
   }
   ```

3. **Access in tests:**

   ```python
   from pytest_testconfig import config as py_config

   # Access test parameters
   plan_config = py_config["tests_params"]["test_my_new_feature"]

   # Use in parametrize
   @pytest.mark.parametrize(
       "plan",
       [pytest.param(py_config["tests_params"]["test_my_new_feature"])],
       indirect=True,
   )
   ```

4. **Access global settings:**

   ```python
   # Global configuration
   namespace_prefix = py_config["target_namespace_prefix"]
   plan_timeout = py_config["plan_wait_timeout"]
   mtv_namespace = py_config["mtv_namespace"]
   ```

**Configuration Requirements (MANDATORY):**

- ✅ **MUST** use descriptive test parameter names matching test function names
- ✅ **MUST** specify required VM properties (name is required)
- ✅ **MUST** set `warm_migration` explicitly (True/False, no defaults)
- ✅ **MUST** use type annotations for global settings
- ❌ **NEVER** modify the auto-export loop at the bottom
- ❌ **NEVER** add non-serializable types to config

### Fixture Patterns

**This project uses pytest fixtures extensively for resource management.**

#### CRITICAL: conftest.py Structure

**`conftest.py` files ONLY accept pytest fixtures and pytest hooks. No standalone functions allowed.**

**What belongs in conftest.py:**

- ✅ **Pytest fixtures** - Functions decorated with `@pytest.fixture`
- ✅ **Pytest hooks** - Functions like `pytest_addoption`, `pytest_runtest_makereport`,
  `pytest_sessionstart`, `pytest_configure`, `pytest_collection_modifyitems`, etc.

**What does NOT belong in conftest.py:**

- ❌ **Standalone helper functions** - Move to `utilities/` directory
- ❌ **Utility functions** - Move to appropriate utility module
- ❌ **Constants or configuration** - Move to `tests/tests_config/config.py`
- ❌ **Classes** (unless pytest plugin classes) - Move to `libs/` or `utilities/`

**Examples:**

```python
# ✅ CORRECT - conftest.py content
import pytest

@pytest.fixture(scope="session")
def ocp_admin_client():
    """Pytest fixture - belongs in conftest.py"""
    return get_cluster_client()

def pytest_addoption(parser):
    """Pytest hook - belongs in conftest.py"""
    parser.addoption("--provider", action="store", default="vsphere")

def pytest_runtest_makereport(item, call):
    """Pytest hook - belongs in conftest.py"""
    if call.excinfo is not None:
        collect_must_gather(item)
```

```python
# ❌ WRONG - These do NOT belong in conftest.py
def get_vm_details(provider, vm_name):
    """Helper function - move to utilities/"""
    ...

def validate_migration_status(plan):
    """Utility function - move to utilities/"""
    ...

MAX_RETRIES = 3  # Constant - move to config
```

**Where to put helper functions:**

| Function Type       | Location                      |
| ------------------- | ----------------------------- |
| Migration utilities | `utilities/mtv_migration.py`  |
| Resource utilities  | `utilities/resources.py`      |
| Provider utilities  | `utilities/providers.py`      |
| General utilities   | `utilities/utils.py`          |
| Provider classes    | `libs/<provider>.py`          |

**Why this rule exists:**

- `conftest.py` has special meaning in pytest - it's for fixtures and hooks only
- Mixing helper functions makes the file harder to maintain
- Helper functions in conftest.py cannot be easily imported by other modules
- Utility modules provide better code organization and reusability

#### Session-Scoped Fixtures

**Used for resources shared across ALL tests in a session:**

```python
@pytest.fixture(scope="session")
def ocp_admin_client():
    """Single OpenShift client for entire test session"""
    return get_cluster_client()

@pytest.fixture(scope="session")
def session_uuid(fixture_store):
    """Unique identifier for this test session"""
    _uuid = generate_name_with_uuid(name="auto")
    fixture_store["session_uuid"] = _uuid
    return _uuid

@pytest.fixture(scope="session")
def source_provider(fixture_store, target_namespace, ...):
    """Single provider connection for entire session"""
    with create_source_provider(
        source_provider_data=source_provider_data,
        namespace=target_namespace,
        admin_client=ocp_admin_client,
        session_uuid=fixture_store["session_uuid"],
        fixture_store=fixture_store,
    ) as _source_provider:
        yield _source_provider
    _source_provider.disconnect()
```

**Common session fixtures:**

- `ocp_admin_client` - OpenShift DynamicClient
- `session_uuid` - Unique session identifier
- `target_namespace` - Test namespace
- `source_provider` - Source provider connection
- `destination_provider` - Destination provider connection
- `fixture_store` - Resource tracking dictionary

#### Function-Scoped Fixtures

**Used for resources created per test:**

```python
@pytest.fixture(scope="function")
def plan(request, fixture_store, source_provider, ...):
    """New migration plan for each test"""
    # Get plan from parametrize
    plan: dict[str, Any] = request.param
    virtual_machines: list[dict[str, Any]] = plan["virtual_machines"]

    # Clone VMs for testing (don't modify source)
    for vm in virtual_machines:
        source_vm_details = source_provider.vm_dict(
            name=vm["name"],
            clone=True,
            session_uuid=fixture_store["session_uuid"],
        )
        vm["name"] = source_vm_details["name"]  # Update with cloned name

    yield plan

    # Teardown: track resources for cleanup
    for vm in plan["virtual_machines"]:
        fixture_store["teardown"].setdefault(VirtualMachine.kind, []).append({
            "name": vm["name"],
            "namespace": target_namespace,
        })
```

#### fixture_store Pattern

**Used for cross-fixture communication and resource tracking:**

```python
# Store data for other fixtures to access
@pytest.fixture(scope="session")
def base_resource_name(fixture_store, session_uuid, source_provider_data):
    name = f"{session_uuid}-{source_provider_data['type']}"
    fixture_store["base_resource_name"] = name  # Store for later use
    return name

# Access stored data from another fixture
@pytest.fixture(scope="function")
def my_resource(fixture_store, ocp_admin_client):
    # Access base_resource_name from fixture_store
    base_name = fixture_store["base_resource_name"]

    resource = create_and_store_resource(
        fixture_store=fixture_store,
        resource=MyResource,
        client=ocp_admin_client,
        name=f"{base_name}-my-resource",
    )
    return resource
```

**fixture_store Structure:**

```python
{
    "session_uuid": "auto-abc123",
    "base_resource_name": "auto-abc123-vsphere-8-0",
    "vms_for_current_session": {"vm1", "vm2", "vm3"},
    "teardown": {
        "Namespace": [{"name": "ns1", ...}],
        "Provider": [{"name": "provider1", ...}],
        "VirtualMachine": [{"name": "vm1", ...}],
    },
}
```

**Fixture Requirements (MANDATORY):**

- ✅ **MUST** use session scope for shared, expensive resources (providers, clients)
- ✅ **MUST** use function scope for test-specific resources (plans, VMs)
- ✅ **MUST** store cross-fixture data in `fixture_store`
- ✅ **MUST** track ALL resources in `fixture_store["teardown"]` for cleanup
- ❌ **NEVER** create resources without `create_and_store_resource()`
- ❌ **NEVER** modify shared session fixtures in tests

### Test Markers and Categorization

**Pytest markers are used to categorize and selectively run tests.**

**Available Markers (defined in `pytest.ini`):**

| Marker        | Purpose                          | Example                         |
| ------------- | -------------------------------- | ------------------------------- |
| `tier0`       | Core functionality (smoke tests) | Basic cold migration            |
| `warm`        | Warm migration tests             | Incremental snapshot migrations |
| `remote`      | Remote cluster tests             | Cross-cluster migrations        |
| `copyoffload` | Copy-offload (XCOPY) tests       | VMware copyoffload              |

**How to Apply Markers:**

```python
# Single marker
@pytest.mark.tier0
def test_basic_migration(...):
    pass

# Multiple markers
@pytest.mark.tier0
@pytest.mark.warm
def test_warm_migration(...):
    pass

# Conditional skip
@pytest.mark.tier0
@pytest.mark.remote
@pytest.mark.skipif(
    not get_value_from_py_config("remote_ocp_cluster"),
    reason="No remote OCP cluster provided"
)
def test_remote_migration(...):
    pass

# Copy-offload test
@pytest.mark.tier0
@pytest.mark.copyoffload
def test_copyoffload_migration(...):
    pass
```

**Marker Requirements:**

- ✅ **OPTIONAL:** `tier0` marker for core functionality tests
- ✅ **OPTIONAL:** Add feature markers (warm, remote, copyoffload)
- ❌ **DON'T:** Use undefined markers (must be in pytest.ini)

### Parallel Execution Support

**Tests support parallel execution using pytest-xdist.**

**Why Parallel Execution is Safe:**

1. **Unique namespaces per session:**
   - Each test session gets a unique UUID via `session_uuid` fixture
   - Namespace pattern: `{target_namespace_prefix}-{session_uuid}`
   - Prevents resource conflicts between parallel runs

2. **Resource isolation:**
   - Each worker has its own `fixture_store`
   - Resources tracked independently per worker
   - Cleanup happens per worker

3. **Thread-safe patterns:**
   - ✅ Unique resource names (UUID-based via `create_and_store_resource()`)
   - ✅ Isolated namespaces per worker
   - ✅ Independent fixture stores
   - ✅ No shared mutable state between tests

**Key Takeaway for Code:**

- Always use fixtures for namespaces (never hardcode)
- Always use `create_and_store_resource()` for unique names
- Never share mutable state between tests
- Session fixtures create expensive resources once, reused across tests in same worker
